# Chat UI for Local Ollama Models

A simple web-based chat interface built with vanilla JavaScript, HTML, and CSS to interact with local Ollama models.

## Features
- Send messages to a local Ollama AI model
- Receive responses in real-time
- See selected model info

## Usage
1. Clone the repository
2. Make sure your Ollama server is running locally. This project communicates with the server to detect models and handle chat.
3. Open `index.html` in your browser
4. Start chatting!

